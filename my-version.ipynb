{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# CELL 1: SETUP & INSTALL (Kaggle Version)\n# ============================================\nimport os\nimport sys\n\n# Install required packages\n!pip install roboflow -q\n!pip install segmentation-models-pytorch -q\n!pip install albumentations -q\n!pip install gradio -q\n!pip install pycocotools -q\n!pip install torchmetrics -q\n!pip install pandas tabulate -q\n\nprint(\"‚úÖ Kaggle setup complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:23:01.680169Z","iopub.execute_input":"2025-12-08T21:23:01.680369Z","iopub.status.idle":"2025-12-08T21:25:02.756574Z","shell.execute_reply.started":"2025-12-08T21:23:01.680348Z","shell.execute_reply":"2025-12-08T21:25:02.755768Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# CELL 2: DOWNLOAD DATASET\n# ============================================\nfrom roboflow import Roboflow\n\n# Your API key\nrf = Roboflow(api_key=\"FoHdZwbhLlvtF4Xo4zdZ\")\nproject = rf.workspace(\"studentdatasets\").project(\"microscopy-cell-segmentation\")\nversion = project.version(21)\ndataset = version.download(\"coco-segmentation\")\n\nprint(\"‚úÖ Dataset downloaded!\")\ndataset_path = dataset.location","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:25:11.065668Z","iopub.execute_input":"2025-12-08T21:25:11.066363Z","iopub.status.idle":"2025-12-08T21:25:19.910453Z","shell.execute_reply.started":"2025-12-08T21:25:11.066322Z","shell.execute_reply":"2025-12-08T21:25:19.909791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# CELL 3: IMPORTS & GPU SETUP\n# ============================================\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport json\nimport os\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport segmentation_models_pytorch as smp\nimport pandas as pd\nimport torch.nn.functional as F\nfrom torchmetrics.classification import BinaryJaccardIndex, BinaryF1Score\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n\n# Check GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"üöÄ Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n    \n# Create output directory\noutput_dir = '/kaggle/working/cell_segmentation_v1'\nos.makedirs(output_dir, exist_ok=True)\nprint(f\"üìÅ Output directory: {output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:25:23.806989Z","iopub.execute_input":"2025-12-08T21:25:23.807569Z","iopub.status.idle":"2025-12-08T21:25:44.344323Z","shell.execute_reply.started":"2025-12-08T21:25:23.807545Z","shell.execute_reply":"2025-12-08T21:25:44.343641Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# CELL 4: IMPROVED DATASET CLASS\n# ============================================\nclass CellSegmentationDataset(Dataset):\n    \"\"\"Improved dataset with better augmentation\"\"\"\n    def __init__(self, json_path, img_dir, img_size=512, augment=True):\n        with open(json_path) as f:\n            data = json.load(f)\n        \n        self.images = data['images']\n        self.annotations = data['annotations']\n        self.img_dir = img_dir\n        self.img_size = img_size\n        self.augment = augment\n        \n        # Create annotation mapping\n        self.ann_map = {}\n        for ann in self.annotations:\n            img_id = ann['image_id']\n            if img_id not in self.ann_map:\n                self.ann_map[img_id] = []\n            self.ann_map[img_id].append(ann)\n        \n        self.image_paths = [os.path.join(img_dir, img['file_name']) for img in self.images]\n        \n        # Enhanced augmentations for microscopy\n        if augment:\n            self.transform = A.Compose([\n                A.Resize(img_size, img_size, always_apply=True),\n                A.HorizontalFlip(p=0.5),\n                A.VerticalFlip(p=0.5),\n                A.RandomRotate90(p=0.5),\n                A.RandomBrightnessContrast(p=0.3, brightness_limit=0.1, contrast_limit=0.1),\n                A.GaussianBlur(p=0.1, blur_limit=(3, 7)),\n                A.GaussNoise(p=0.1, var_limit=(10.0, 50.0)),\n                A.ElasticTransform(p=0.2, alpha=1, sigma=50, alpha_affine=50),\n                A.CoarseDropout(p=0.1, max_holes=8, max_height=32, max_width=32, fill_value=0),\n                A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n                ToTensorV2(),\n            ])\n        else:\n            self.transform = A.Compose([\n                A.Resize(img_size, img_size, always_apply=True),\n                A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n                ToTensorV2(),\n            ])\n    \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img_info = self.images[idx]\n        \n        # Create mask\n        mask = np.zeros((self.img_size, self.img_size), dtype=np.float32)\n        \n        if img_info['id'] in self.ann_map:\n            for ann in self.ann_map[img_info['id']]:\n                for seg in ann['segmentation']:\n                    pts = np.array(seg).reshape(-1, 2)\n                    if len(pts) > 0:\n                        # Preserve aspect ratio\n                        pts[:, 0] = pts[:, 0] * self.img_size / img_info['width']\n                        pts[:, 1] = pts[:, 1] * self.img_size / img_info['height']\n                        pts = pts.astype(np.int32)\n                        cv2.fillPoly(mask, [pts], 1)\n        \n        transformed = self.transform(image=img, mask=mask)\n        img_tensor = transformed['image']\n        mask_tensor = transformed['mask']\n        \n        return img_tensor, mask_tensor.float()\n\n# Create datasets\nprint(\"üìä Creating datasets...\")\ntrain_dataset = CellSegmentationDataset(\n    os.path.join(dataset_path, \"train\", \"_annotations.coco.json\"),\n    os.path.join(dataset_path, \"train\"),\n    augment=True\n)\n\nval_dataset = CellSegmentationDataset(\n    os.path.join(dataset_path, \"valid\", \"_annotations.coco.json\"),\n    os.path.join(dataset_path, \"valid\"),\n    augment=False\n)\n\ntest_dataset = CellSegmentationDataset(\n    os.path.join(dataset_path, \"test\", \"_annotations.coco.json\"),\n    os.path.join(dataset_path, \"test\"),\n    augment=False\n)\n\nprint(f\"‚úÖ Datasets created!\")\nprint(f\"Train: {len(train_dataset)} images\")\nprint(f\"Validation: {len(val_dataset)} images\")\nprint(f\"Test: {len(test_dataset)} images\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:25:50.565242Z","iopub.execute_input":"2025-12-08T21:25:50.566000Z","iopub.status.idle":"2025-12-08T21:25:51.573219Z","shell.execute_reply.started":"2025-12-08T21:25:50.565973Z","shell.execute_reply":"2025-12-08T21:25:51.572471Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# CELL 5: CREATE 2 MODELS (Version 1)\n# ============================================\nprint(\"üß† CREATING 2 MODELS FOR VERSION 1...\")\nprint(\"=\"*50)\n\n# 1. U-Net with EfficientNet-B4\nprint(\"1. Creating U-Net EfficientNet-B4...\")\nmodel1 = smp.Unet(\n    encoder_name=\"timm-efficientnet-b4\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=1,\n    activation=None,\n    decoder_attention_type=\"scse\",\n    decoder_dropout=0.3\n).to(device)\n\n# 2. DeepLabV3+ with ResNet50\nprint(\"2. Creating DeepLabV3+ ResNet50...\")\nmodel2 = smp.DeepLabV3Plus(\n    encoder_name=\"resnet50\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=1,\n    activation=None,\n    decoder_dropout=0.2\n).to(device)\n\nmodels_v1 = {\n    'unet_effb4': model1,\n    'deeplabv3_r50': model2\n}\n\nfor name, model in models_v1.items():\n    params = sum(p.numel() for p in model.parameters()) / 1e6\n    print(f\"{name}: {params:.1f}M parameters\")\nprint(\"‚úÖ 2 Models created for Version 1\")\nprint(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:26:01.498172Z","iopub.execute_input":"2025-12-08T21:26:01.498738Z","iopub.status.idle":"2025-12-08T21:26:06.855855Z","shell.execute_reply.started":"2025-12-08T21:26:01.498713Z","shell.execute_reply":"2025-12-08T21:26:06.855110Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# CELL 6: ENHANCED TRAINER WITH COMPREHENSIVE METRICS\n# ============================================\nclass EnhancedTrainer:\n    def __init__(self, device='cuda'):\n        self.device = device\n        self.bce_loss = nn.BCEWithLogitsLoss()\n        self.dice_loss = smp.losses.DiceLoss(mode='binary')\n        self.focal_loss = smp.losses.FocalLoss(mode='binary')\n        \n        # Comprehensive metrics\n        self.iou_metric = BinaryJaccardIndex().to(device)\n        self.f1_metric = BinaryF1Score().to(device)\n    \n    def create_dataloaders(self, batch_size=8):\n        \"\"\"Create dataloaders\"\"\"\n        train_loader = DataLoader(\n            train_dataset, \n            batch_size=batch_size, \n            shuffle=True,\n            num_workers=2,\n            pin_memory=True\n        )\n        \n        val_loader = DataLoader(\n            val_dataset, \n            batch_size=batch_size, \n            shuffle=False,\n            num_workers=2,\n            pin_memory=True\n        )\n        \n        test_loader = DataLoader(\n            test_dataset, \n            batch_size=batch_size, \n            shuffle=False,\n            num_workers=2,\n            pin_memory=True\n        )\n        \n        return train_loader, val_loader, test_loader\n    \n    def calculate_comprehensive_metrics(self, outputs, targets, threshold=0.5):\n        \"\"\"Calculate all metrics for segmentation\"\"\"\n        with torch.no_grad():\n            preds = torch.sigmoid(outputs)\n            preds_binary = (preds > threshold).float()\n            \n            # Basic metrics\n            iou = self.iou_metric(preds_binary, targets)\n            f1 = self.f1_metric(preds_binary, targets)\n            \n            # Additional metrics\n            intersection = (preds_binary * targets).sum()\n            union = preds_binary.sum() + targets.sum()\n            dice = (2 * intersection) / (union + 1e-7)\n            \n            tp = (preds_binary * targets).sum()\n            fp = (preds_binary * (1 - targets)).sum()\n            fn = ((1 - preds_binary) * targets).sum()\n            tn = ((1 - preds_binary) * (1 - targets)).sum()\n            \n            precision = tp / (tp + fp + 1e-7)\n            recall = tp / (tp + fn + 1e-7)\n            specificity = tn / (tn + fp + 1e-7)\n            accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-7)\n            \n            return {\n                'iou': iou.item(),\n                'f1': f1.item(),\n                'dice': dice.item(),\n                'precision': precision.item(),\n                'recall': recall.item(),\n                'specificity': specificity.item(),\n                'accuracy': accuracy.item(),\n                'tp': tp.item(),\n                'fp': fp.item(),\n                'fn': fn.item(),\n                'tn': tn.item()\n            }\n    \n    def combined_loss(self, outputs, targets):\n        \"\"\"Weighted combination of multiple losses\"\"\"\n        bce = self.bce_loss(outputs, targets)\n        dice = self.dice_loss(outputs, targets)\n        focal = self.focal_loss(outputs, targets)\n        return 0.4*bce + 0.4*dice + 0.2*focal\n    \n    def train_epoch(self, model, loader, optimizer, scaler=None, epoch=None):\n        \"\"\"Train for one epoch with comprehensive metrics\"\"\"\n        model.train()\n        epoch_metrics = {\n            'loss': 0, 'iou': 0, 'f1': 0, 'dice': 0,\n            'precision': 0, 'recall': 0, 'accuracy': 0\n        }\n        \n        pbar = tqdm(loader, desc=f'Training Epoch {epoch+1}')\n        for images, masks in pbar:\n            images, masks = images.to(self.device), masks.to(self.device).unsqueeze(1)\n            \n            optimizer.zero_grad()\n            \n            if scaler is not None:\n                with torch.cuda.amp.autocast():\n                    outputs = model(images)\n                    loss = self.combined_loss(outputs, masks)\n                \n                scaler.scale(loss).backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                outputs = model(images)\n                loss = self.combined_loss(outputs, masks)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                optimizer.step()\n            \n            # Calculate metrics\n            metrics = self.calculate_comprehensive_metrics(outputs, masks)\n            \n            # Update epoch metrics\n            epoch_metrics['loss'] += loss.item()\n            for key in metrics:\n                if key in epoch_metrics:\n                    epoch_metrics[key] += metrics[key]\n            \n            # Update progress bar\n            pbar.set_postfix({\n                'loss': f\"{loss.item():.4f}\",\n                'iou': f\"{metrics['iou']:.4f}\",\n                'dice': f\"{metrics['dice']:.4f}\"\n            })\n        \n        # Average metrics\n        for key in epoch_metrics:\n            epoch_metrics[key] /= len(loader)\n        \n        return epoch_metrics\n    \n    def validate(self, model, loader, split='Validation'):\n        \"\"\"Validate model with comprehensive metrics\"\"\"\n        model.eval()\n        val_metrics = {\n            'loss': 0, 'iou': 0, 'f1': 0, 'dice': 0,\n            'precision': 0, 'recall': 0, 'accuracy': 0\n        }\n        \n        with torch.no_grad():\n            for images, masks in tqdm(loader, desc=split):\n                images, masks = images.to(self.device), masks.to(self.device).unsqueeze(1)\n                outputs = model(images)\n                \n                loss = self.combined_loss(outputs, masks)\n                metrics = self.calculate_comprehensive_metrics(outputs, masks)\n                \n                val_metrics['loss'] += loss.item()\n                for key in metrics:\n                    if key in val_metrics:\n                        val_metrics[key] += metrics[key]\n        \n        # Average metrics\n        for key in val_metrics:\n            val_metrics[key] /= len(loader)\n        \n        return val_metrics\n    \n    def train_model(self, model, train_loader, val_loader, model_name, \n                   epochs=30, lr=1e-4, patience=10):\n        \"\"\"Complete training with comprehensive tracking\"\"\"\n        print(f\"\\nüöÄ Training {model_name} for {epochs} epochs...\")\n        print(\"=\"*60)\n        \n        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='max', factor=0.5, patience=5, verbose=True, min_lr=1e-6\n        )\n        \n        scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n        \n        # Comprehensive history\n        history = {\n            'train': {k: [] for k in ['loss', 'iou', 'f1', 'dice', 'precision', 'recall', 'accuracy']},\n            'val': {k: [] for k in ['loss', 'iou', 'f1', 'dice', 'precision', 'recall', 'accuracy']},\n            'lr': [],\n            'best_epoch': 0\n        }\n        \n        best_iou = 0\n        patience_counter = 0\n        best_model_state = None\n        \n        for epoch in range(epochs):\n            print(f\"\\n{'='*60}\")\n            print(f\"Epoch {epoch+1}/{epochs}\")\n            print('='*60)\n            \n            # Training\n            train_metrics = self.train_epoch(model, train_loader, optimizer, scaler, epoch)\n            for key in train_metrics:\n                history['train'][key].append(train_metrics[key])\n            \n            # Validation\n            val_metrics = self.validate(model, val_loader, 'Validation')\n            for key in val_metrics:\n                history['val'][key].append(val_metrics[key])\n            \n            # Learning rate tracking\n            current_lr = optimizer.param_groups[0]['lr']\n            history['lr'].append(current_lr)\n            \n            # Print epoch results\n            print(f\"Train - Loss: {train_metrics['loss']:.4f}, IoU: {train_metrics['iou']:.4f}, Dice: {train_metrics['dice']:.4f}\")\n            print(f\"Val   - Loss: {val_metrics['loss']:.4f}, IoU: {val_metrics['iou']:.4f}, Dice: {val_metrics['dice']:.4f}\")\n            print(f\"Metrics - Precision: {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}, Accuracy: {val_metrics['accuracy']:.4f}\")\n            print(f\"Learning Rate: {current_lr:.6f}\")\n            \n            # Update scheduler\n            scheduler.step(val_metrics['iou'])\n            \n            # Early stopping and model saving\n            if val_metrics['iou'] > best_iou:\n                best_iou = val_metrics['iou']\n                patience_counter = 0\n                history['best_epoch'] = epoch\n                best_model_state = model.state_dict().copy()\n                \n                # Save best model\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'val_iou': best_iou,\n                    'history': history,\n                    'val_metrics': val_metrics\n                }, os.path.join(output_dir, f'{model_name}_best.pth'))\n                print(f\"üíæ Saved best model with IoU: {best_iou:.4f}\")\n            else:\n                patience_counter += 1\n                print(f\"‚è≥ No improvement ({patience_counter}/{patience})\")\n            \n            if patience_counter >= patience:\n                print(f\"‚èπÔ∏è Early stopping at epoch {epoch+1}\")\n                break\n        \n        # Restore best model\n        if best_model_state is not None:\n            model.load_state_dict(best_model_state)\n        \n        # Save final model and history\n        torch.save({\n            'model_state_dict': model.state_dict(),\n            'history': history,\n            'best_iou': best_iou,\n            'final_epoch': epoch\n        }, os.path.join(output_dir, f'{model_name}_final.pth'))\n        \n        # Save metrics to CSV\n        metrics_df = pd.DataFrame({\n            'epoch': list(range(1, len(history['train']['loss']) + 1)),\n            'train_loss': history['train']['loss'],\n            'val_loss': history['val']['loss'],\n            'train_iou': history['train']['iou'],\n            'val_iou': history['val']['iou'],\n            'train_dice': history['train']['dice'],\n            'val_dice': history['val']['dice'],\n            'val_precision': history['val']['precision'],\n            'val_recall': history['val']['recall'],\n            'val_accuracy': history['val']['accuracy'],\n            'learning_rate': history['lr']\n        })\n        metrics_df.to_csv(os.path.join(output_dir, f'{model_name}_metrics.csv'), index=False)\n        \n        print(f\"\\n‚úÖ Training completed for {model_name}!\")\n        print(f\"üìä Best Validation IoU: {best_iou:.4f} at epoch {history['best_epoch'] + 1}\")\n        print(f\"üíæ Models saved to: {output_dir}\")\n        \n        return history, best_iou\n\n# Initialize trainer\ntrainer = EnhancedTrainer(device=device)\nprint(\"‚úÖ Enhanced trainer created with comprehensive metrics!\")\n\n# Create dataloaders\ntrain_loader, val_loader, test_loader = trainer.create_dataloaders(batch_size=8)\nprint(f\"üìä Dataloaders created:\")\nprint(f\"   Train batches: {len(train_loader)}\")\nprint(f\"   Val batches: {len(val_loader)}\")\nprint(f\"   Test batches: {len(test_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:26:13.363398Z","iopub.execute_input":"2025-12-08T21:26:13.363948Z","iopub.status.idle":"2025-12-08T21:26:13.396384Z","shell.execute_reply.started":"2025-12-08T21:26:13.363921Z","shell.execute_reply":"2025-12-08T21:26:13.395600Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# CELL 7a: TRAIN MODEL 1 - U-Net EfficientNet-B4 (30 EPOCHS)\n# ============================================\nprint(\"=\"*60)\nprint(\"1. TRAINING: U-Net EfficientNet-B4 (30 EPOCHS)\")\nprint(\"=\"*60)\n\n# Initialize results storage if not exists\nif 'all_results_v1' not in globals():\n    all_results_v1 = {}\n\n# Train Model 1\nhistory1, best_iou1 = trainer.train_model(\n    model=model1,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    model_name=\"unet_effb4\",\n    epochs=30,  # Increased to 30\n    lr=1e-4,\n    patience=10\n)\n\nall_results_v1['unet_effb4'] = {\n    'history': history1,\n    'best_iou': best_iou1,\n    'model': model1,\n    'params': f\"{sum(p.numel() for p in model1.parameters()) / 1e6:.1f}M\"\n}\n\nprint(f\"\\n‚úÖ Model 1 Training Completed!\")\nprint(f\"üìä Best Validation IoU: {best_iou1:.4f}\")\nprint(f\"üî¢ Parameters: {all_results_v1['unet_effb4']['params']}\")\n\n# Save individual model checkpoint\ntorch.save({\n    'model_state_dict': model1.state_dict(),\n    'best_iou': best_iou1,\n    'history': history1,\n    'epochs': 30\n}, os.path.join(output_dir, 'model1_complete.pth'))\n\nprint(f\"üíæ Model 1 saved to: {output_dir}/model1_complete.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:26:25.387299Z","iopub.execute_input":"2025-12-08T21:26:25.388037Z","iopub.status.idle":"2025-12-08T21:27:19.406052Z","shell.execute_reply.started":"2025-12-08T21:26:25.388009Z","shell.execute_reply":"2025-12-08T21:27:19.404867Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# CELL 7b: TRAIN MODEL 2 - DeepLabV3+ ResNet50 (30 EPOCHS)\n# ============================================\nprint(\"=\"*60)\nprint(\"2. TRAINING: DeepLabV3+ ResNet50 (30 EPOCHS)\")\nprint(\"=\"*60)\n\n# Train Model 2\nhistory2, best_iou2 = trainer.train_model(\n    model=model2,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    model_name=\"deeplabv3_r50\",\n    epochs=30,  # Increased to 30\n    lr=1e-4,\n    patience=10\n)\n\nall_results_v1['deeplabv3_r50'] = {\n    'history': history2,\n    'best_iou': best_iou2,\n    'model': model2,\n    'params': f\"{sum(p.numel() for p in model2.parameters()) / 1e6:.1f}M\"\n}\n\nprint(f\"\\n‚úÖ Model 2 Training Completed!\")\nprint(f\"üìä Best Validation IoU: {best_iou2:.4f}\")\nprint(f\"üî¢ Parameters: {all_results_v1['deeplabv3_r50']['params']}\")\n\n# Save individual model checkpoint\ntorch.save({\n    'model_state_dict': model2.state_dict(),\n    'best_iou': best_iou2,\n    'history': history2,\n    'epochs': 30\n}, os.path.join(output_dir, 'model2_complete.pth'))\n\nprint(f\"üíæ Model 2 saved to: {output_dir}/model2_complete.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:27:24.213927Z","iopub.execute_input":"2025-12-08T21:27:24.214227Z","iopub.status.idle":"2025-12-08T21:27:28.881749Z","shell.execute_reply.started":"2025-12-08T21:27:24.214202Z","shell.execute_reply":"2025-12-08T21:27:28.880632Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# CELL 7c: VERSION 1 TRAINING SUMMARY\n# ============================================\nprint(\"=\"*60)\nprint(\"‚úÖ VERSION 1 TRAINING COMPLETED!\")\nprint(\"=\"*60)\n\nprint(\"\\nüìä TRAINING SUMMARY:\")\nprint(\"-\" * 40)\nfor name, data in all_results_v1.items():\n    print(f\"Model: {name}\")\n    print(f\"  ‚Ä¢ Best Validation IoU: {data['best_iou']:.4f}\")\n    print(f\"  ‚Ä¢ Parameters: {data['params']}\")\n    \n    # Show some training history\n    if 'history' in data and 'val' in data['history']:\n        val_history = data['history']['val']\n        if 'iou' in val_history and len(val_history['iou']) > 0:\n            print(f\"  ‚Ä¢ Final Epoch IoU: {val_history['iou'][-1]:.4f}\")\n            print(f\"  ‚Ä¢ Best Epoch: {data['history']['best_epoch'] + 1}\")\n    print(\"-\" * 40)\n\n# Save comprehensive results summary\nimport json\nsummary_v1 = {\n    'version': 'VERSION_1_MODELS_1_2',\n    'models_trained': list(all_results_v1.keys()),\n    'training_details': {},\n    'timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n    'epochs': 30,\n    'device': str(device)\n}\n\nfor name, data in all_results_v1.items():\n    summary_v1['training_details'][name] = {\n        'best_iou': float(data['best_iou']),\n        'params': data['params'],\n        'final_epoch': len(data['history']['train']['loss']) if 'history' in data else 0\n    }\n\nsummary_path = os.path.join(output_dir, 'version_1_summary.json')\nwith open(summary_path, 'w') as f:\n    json.dump(summary_v1, f, indent=2)\n\nprint(f\"\\nüíæ Version 1 summary saved to: {summary_path}\")\nprint(f\"üìÅ Output directory: {output_dir}\")\n\n# List all saved files\nprint(\"\\nüìã Saved files in output directory:\")\nfor file in os.listdir(output_dir):\n    if file.endswith('.pth') or file.endswith('.json') or file.endswith('.csv'):\n        size = os.path.getsize(os.path.join(output_dir, file)) / 1024\n        print(f\"  ‚Ä¢ {file} ({size:.1f} KB)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:27:35.285074Z","iopub.execute_input":"2025-12-08T21:27:35.285844Z","iopub.status.idle":"2025-12-08T21:27:35.297159Z","shell.execute_reply.started":"2025-12-08T21:27:35.285786Z","shell.execute_reply":"2025-12-08T21:27:35.296558Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# CELL 8: COMPREHENSIVE TEST SET EVALUATION\n# ============================================\nprint(\"=\"*60)\nprint(\"üìä COMPREHENSIVE TEST SET EVALUATION\")\nprint(\"=\"*60)\n\ndef evaluate_model_test(model, test_loader, model_name):\n    \"\"\"Comprehensive test evaluation with per-metric tracking\"\"\"\n    model.eval()\n    \n    # Initialize metrics accumulators\n    metrics_sum = {\n        'loss': 0, 'iou': 0, 'f1': 0, 'dice': 0,\n        'precision': 0, 'recall': 0, 'accuracy': 0,\n        'specificity': 0\n    }\n    \n    # Store per-batch metrics for std calculation\n    batch_metrics = {k: [] for k in metrics_sum.keys()}\n    \n    # Store predictions for visualization\n    sample_predictions = []\n    sample_images = []\n    sample_masks = []\n    \n    with torch.no_grad():\n        for batch_idx, (images, masks) in enumerate(tqdm(test_loader, desc=f'Testing {model_name}')):\n            images, masks = images.to(device), masks.to(device).unsqueeze(1)\n            outputs = model(images)\n            \n            loss = trainer.combined_loss(outputs, masks)\n            metrics = trainer.calculate_comprehensive_metrics(outputs, masks)\n            \n            # Accumulate sums\n            metrics_sum['loss'] += loss.item()\n            for key in metrics:\n                if key in metrics_sum:\n                    metrics_sum[key] += metrics[key]\n                    batch_metrics[key].append(metrics[key])\n            \n            # Store first few samples for visualization\n            if batch_idx < 3:  # Store predictions from first 3 batches\n                preds = torch.sigmoid(outputs)\n                preds_binary = (preds > 0.5).float()\n                \n                for i in range(min(2, len(images))):  # Store 2 samples per batch\n                    sample_images.append(images[i].cpu())\n                    sample_masks.append(masks[i].cpu())\n                    sample_predictions.append({\n                        'prob': preds[i].cpu(),\n                        'binary': preds_binary[i].cpu()\n                    })\n    \n    # Calculate means and standard deviations\n    num_batches = len(test_loader)\n    results_mean = {}\n    results_std = {}\n    \n    for key in metrics_sum:\n        results_mean[key] = metrics_sum[key] / num_batches\n        if batch_metrics[key]:\n            results_std[key] = np.std(batch_metrics[key])\n        else:\n            results_std[key] = 0\n    \n    return results_mean, results_std, sample_images, sample_masks, sample_predictions\n\n# Evaluate both models\nprint(\"\\nüîç Evaluating models on test set...\")\ntest_results_v1 = []\nall_test_metrics = {}\n\nfor model_name, data in all_results_v1.items():\n    print(f\"\\n{'='*40}\")\n    print(f\"Evaluating {model_name}...\")\n    print('='*40)\n    \n    model = data['model']\n    mean_metrics, std_metrics, sample_imgs, sample_msks, sample_preds = evaluate_model_test(\n        model, test_loader, model_name\n    )\n    \n    # Store results\n    test_results_v1.append({\n        'Model': model_name,\n        'Test_IoU_Mean': f\"{mean_metrics['iou']:.4f}\",\n        'Test_IoU_Std': f\"{std_metrics['iou']:.4f}\",\n        'Test_Dice_Mean': f\"{mean_metrics['dice']:.4f}\",\n        'Test_Dice_Std': f\"{std_metrics['dice']:.4f}\",\n        'Test_F1_Mean': f\"{mean_metrics['f1']:.4f}\",\n        'Test_F1_Std': f\"{std_metrics['f1']:.4f}\",\n        'Precision': f\"{mean_metrics['precision']:.4f}\",\n        'Recall': f\"{mean_metrics['recall']:.4f}\",\n        'Accuracy': f\"{mean_metrics['accuracy']:.4f}\",\n        'Specificity': f\"{mean_metrics['specificity']:.4f}\",\n        'Val_IoU_Best': f\"{data['best_iou']:.4f}\",\n        'Parameters': data['params']\n    })\n    \n    # Store detailed metrics for visualization\n    all_test_metrics[model_name] = {\n        'mean': mean_metrics,\n        'std': std_metrics,\n        'samples': {\n            'images': sample_imgs,\n            'masks': sample_msks,\n            'predictions': sample_preds\n        }\n    }\n    \n    # Print detailed metrics\n    print(f\"\\nüìä Test Metrics for {model_name}:\")\n    print(f\"  ‚Ä¢ IoU:        {mean_metrics['iou']:.4f} ¬± {std_metrics['iou']:.4f}\")\n    print(f\"  ‚Ä¢ Dice:       {mean_metrics['dice']:.4f} ¬± {std_metrics['dice']:.4f}\")\n    print(f\"  ‚Ä¢ F1-Score:   {mean_metrics['f1']:.4f} ¬± {std_metrics['f1']:.4f}\")\n    print(f\"  ‚Ä¢ Precision:  {mean_metrics['precision']:.4f}\")\n    print(f\"  ‚Ä¢ Recall:     {mean_metrics['recall']:.4f}\")\n    print(f\"  ‚Ä¢ Accuracy:   {mean_metrics['accuracy']:.4f}\")\n    print(f\"  ‚Ä¢ Specificity: {mean_metrics['specificity']:.4f}\")\n    print(f\"  ‚Ä¢ Loss:       {mean_metrics['loss']:.4f}\")\n    print(f\"  ‚Ä¢ Best Val IoU: {data['best_iou']:.4f}\")\n\n# Display results as table\nprint(\"\\n\" + \"=\"*80)\nprint(\"üèÜ FINAL TEST EVALUATION RESULTS\")\nprint(\"=\"*80)\n\nimport pandas as pd\nfrom tabulate import tabulate\n\nif test_results_v1:\n    df_results = pd.DataFrame(test_results_v1)\n    print(tabulate(df_results, headers='keys', tablefmt='pretty', showindex=False))\n    \n    # Save results to CSV\n    results_csv_path = os.path.join(output_dir, 'test_evaluation_results.csv')\n    df_results.to_csv(results_csv_path, index=False)\n    print(f\"\\nüíæ Results saved to: {results_csv_path}\")\n    \n    # Sort by IoU for ranking\n    df_sorted = df_results.copy()\n    df_sorted['IoU_Value'] = df_sorted['Test_IoU_Mean'].apply(lambda x: float(x))\n    df_sorted = df_sorted.sort_values('IoU_Value', ascending=False)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"üìà RANKING BY TEST IoU (Best to Worst)\")\n    print(\"=\"*80)\n    print(tabulate(df_sorted.drop('IoU_Value', axis=1), \n                  headers='keys', tablefmt='pretty', showindex=False))\nelse:\n    print(\"‚ö†Ô∏è No evaluation results to display\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:27:39.609869Z","iopub.execute_input":"2025-12-08T21:27:39.610675Z","iopub.status.idle":"2025-12-08T21:27:39.647562Z","shell.execute_reply.started":"2025-12-08T21:27:39.610648Z","shell.execute_reply":"2025-12-08T21:27:39.647012Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# CELL 9: COMPREHENSIVE VISUALIZATION\n# ============================================\nprint(\"=\"*60)\nprint(\"üìä COMPREHENSIVE VISUALIZATION\")\nprint(\"=\"*60)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import gridspec\n\n# Set style\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\n# Create visualization directory\nvis_dir = os.path.join(output_dir, 'visualizations')\nos.makedirs(vis_dir, exist_ok=True)\n\ndef visualize_training_history(all_results_dict):\n    \"\"\"Visualize training history for all models\"\"\"\n    fig = plt.figure(figsize=(18, 12))\n    \n    # Create subplots\n    gs = gridspec.GridSpec(3, 3, figure=fig)\n    \n    metrics_to_plot = ['loss', 'iou', 'dice', 'precision', 'recall', 'accuracy']\n    titles = ['Loss', 'IoU', 'Dice Coefficient', 'Precision', 'Recall', 'Accuracy']\n    \n    for idx, (metric, title) in enumerate(zip(metrics_to_plot, titles)):\n        ax = fig.add_subplot(gs[idx // 3, idx % 3])\n        \n        for model_name, data in all_results_dict.items():\n            if 'history' in data and 'train' in data['history']:\n                train_vals = data['history']['train'].get(metric, [])\n                val_vals = data['history']['val'].get(metric, [])\n                \n                if train_vals and val_vals:\n                    epochs = range(1, len(train_vals) + 1)\n                    ax.plot(epochs, train_vals, '--', linewidth=1.5, label=f'{model_name} Train')\n                    ax.plot(epochs, val_vals, '-', linewidth=2, label=f'{model_name} Val')\n        \n        ax.set_xlabel('Epoch')\n        ax.set_ylabel(title)\n        ax.set_title(f'Training vs Validation {title}')\n        ax.legend(fontsize=8)\n        ax.grid(True, alpha=0.3)\n    \n    # Learning rate plot\n    ax_lr = fig.add_subplot(gs[2, :])\n    for model_name, data in all_results_dict.items():\n        if 'history' in data and 'lr' in data['history']:\n            lr_vals = data['history']['lr']\n            if lr_vals:\n                epochs = range(1, len(lr_vals) + 1)\n                ax_lr.plot(epochs, lr_vals, 'o-', linewidth=2, label=model_name)\n    \n    ax_lr.set_xlabel('Epoch')\n    ax_lr.set_ylabel('Learning Rate')\n    ax_lr.set_title('Learning Rate Schedule')\n    ax_lr.set_yscale('log')\n    ax_lr.legend()\n    ax_lr.grid(True, alpha=0.3)\n    \n    plt.suptitle('Model Training History Comparison', fontsize=16, fontweight='bold', y=1.02)\n    plt.tight_layout()\n    \n    save_path = os.path.join(vis_dir, 'training_history_comparison.png')\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    plt.show()\n    print(f\"‚úÖ Training history saved to: {save_path}\")\n\ndef visualize_test_metrics_comparison(test_metrics_dict):\n    \"\"\"Visualize test metrics comparison\"\"\"\n    if not test_metrics_dict:\n        return\n    \n    metrics_to_plot = ['iou', 'dice', 'f1', 'precision', 'recall', 'accuracy']\n    metric_names = ['IoU', 'Dice', 'F1-Score', 'Precision', 'Recall', 'Accuracy']\n    \n    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n    axes = axes.flatten()\n    \n    model_names = list(test_metrics_dict.keys())\n    \n    for idx, (metric, metric_name) in enumerate(zip(metrics_to_plot, metric_names)):\n        ax = axes[idx]\n        \n        means = []\n        stds = []\n        for model_name in model_names:\n            if 'mean' in test_metrics_dict[model_name]:\n                means.append(test_metrics_dict[model_name]['mean'][metric])\n                stds.append(test_metrics_dict[model_name]['std'][metric])\n        \n        if means:\n            x_pos = np.arange(len(model_names))\n            bars = ax.bar(x_pos, means, yerr=stds, capsize=5, alpha=0.7)\n            \n            # Add value labels on top of bars\n            for bar, mean_val in zip(bars, means):\n                height = bar.get_height()\n                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n                       f'{mean_val:.3f}', ha='center', va='bottom', fontsize=9)\n            \n            ax.set_xlabel('Models')\n            ax.set_ylabel(metric_name)\n            ax.set_title(f'Test {metric_name} Comparison')\n            ax.set_xticks(x_pos)\n            ax.set_xticklabels([name[:15] for name in model_names], rotation=45, ha='right')\n            ax.grid(True, alpha=0.3, axis='y')\n    \n    plt.suptitle('Model Performance Comparison on Test Set', fontsize=16, fontweight='bold', y=1.02)\n    plt.tight_layout()\n    \n    save_path = os.path.join(vis_dir, 'test_metrics_comparison.png')\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    plt.show()\n    print(f\"‚úÖ Test metrics comparison saved to: {save_path}\")\n\ndef visualize_sample_predictions(test_metrics_dict, num_samples=3):\n    \"\"\"Visualize sample predictions from all models\"\"\"\n    if not test_metrics_dict:\n        return\n    \n    # Get sample data from first model\n    first_model = list(test_metrics_dict.keys())[0]\n    if 'samples' not in test_metrics_dict[first_model]:\n        return\n    \n    samples = test_metrics_dict[first_model]['samples']\n    num_models = len(test_metrics_dict)\n    \n    fig, axes = plt.subplots(num_samples, num_models + 2, figsize=(5*(num_models+2), 4*num_samples))\n    \n    if num_samples == 1:\n        axes = axes.reshape(1, -1)\n    \n    for sample_idx in range(min(num_samples, len(samples['images']))):\n        img = samples['images'][sample_idx]\n        true_mask = samples['masks'][sample_idx]\n        \n        # Denormalize image\n        img_np = img.numpy().transpose(1, 2, 0)\n        img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n        img_np = np.clip(img_np, 0, 1)\n        \n        true_mask_np = true_mask.numpy().squeeze()\n        \n        # Original image\n        ax = axes[sample_idx, 0] if num_samples > 1 else axes[0]\n        ax.imshow(img_np)\n        ax.set_title('Original Image', fontsize=10, fontweight='bold')\n        ax.axis('off')\n        \n        # Ground truth\n        ax = axes[sample_idx, 1] if num_samples > 1 else axes[1]\n        ax.imshow(true_mask_np, cmap='gray')\n        ax.set_title('Ground Truth', fontsize=10, fontweight='bold')\n        ax.axis('off')\n        \n        # Each model's prediction\n        for model_idx, (model_name, model_data) in enumerate(test_metrics_dict.items()):\n            if 'samples' in model_data and len(model_data['samples']['predictions']) > sample_idx:\n                pred_data = model_data['samples']['predictions'][sample_idx]\n                pred_binary = pred_data['binary'].numpy().squeeze()\n                \n                # Calculate Dice for this sample\n                intersection = (pred_binary * true_mask_np).sum()\n                union = pred_binary.sum() + true_mask_np.sum()\n                dice = (2 * intersection) / (union + 1e-7) if union > 0 else 0\n                \n                col_idx = model_idx + 2\n                ax = axes[sample_idx, col_idx] if num_samples > 1 else axes[col_idx]\n                ax.imshow(pred_binary, cmap='gray')\n                ax.set_title(f'{model_name}\\nDice: {dice:.3f}', fontsize=9)\n                ax.axis('off')\n    \n    plt.suptitle('Model Predictions Comparison on Sample Images', fontsize=16, fontweight='bold', y=1.02)\n    plt.tight_layout()\n    \n    save_path = os.path.join(vis_dir, 'sample_predictions_comparison.png')\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    plt.show()\n    print(f\"‚úÖ Sample predictions saved to: {save_path}\")\n\ndef create_confusion_matrix_heatmap(test_metrics_dict):\n    \"\"\"Create confusion matrix visualization\"\"\"\n    if not test_metrics_dict:\n        return\n    \n    fig, axes = plt.subplots(1, len(test_metrics_dict), figsize=(5*len(test_metrics_dict), 4))\n    \n    if len(test_metrics_dict) == 1:\n        axes = [axes]\n    \n    for idx, (model_name, model_data) in enumerate(test_metrics_dict.items()):\n        if 'mean' in model_data:\n            ax = axes[idx]\n            \n            # Get confusion matrix components\n            tp = model_data['mean'].get('tp', 0)\n            fp = model_data['mean'].get('fp', 0)\n            fn = model_data['mean'].get('fn', 0)\n            tn = model_data['mean'].get('tn', 0)\n            \n            cm = np.array([[tn, fp], [fn, tp]])\n            \n            # Normalize by row\n            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n            \n            # Create heatmap\n            im = ax.imshow(cm_normalized, interpolation='nearest', cmap='Blues', vmin=0, vmax=1)\n            \n            # Add text annotations\n            for i in range(2):\n                for j in range(2):\n                    text = ax.text(j, i, f\"{cm[i, j]}\\n({cm_normalized[i, j]:.2%})\",\n                                 ha=\"center\", va=\"center\", color=\"black\" if cm_normalized[i, j] < 0.7 else \"white\")\n            \n            ax.set_title(f'{model_name}\\nConfusion Matrix', fontsize=11)\n            ax.set_xlabel('Predicted')\n            ax.set_ylabel('Actual')\n            ax.set_xticks([0, 1])\n            ax.set_yticks([0, 1])\n            ax.set_xticklabels(['Negative', 'Positive'])\n            ax.set_yticklabels(['Negative', 'Positive'])\n    \n    plt.suptitle('Confusion Matrices Comparison', fontsize=14, fontweight='bold', y=1.05)\n    plt.tight_layout()\n    \n    save_path = os.path.join(vis_dir, 'confusion_matrices.png')\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    plt.show()\n    print(f\"‚úÖ Confusion matrices saved to: {save_path}\")\n\n# Run all visualizations\nprint(\"\\nüìà Generating visualizations...\")\n\n# 1. Training history\nif all_results_v1:\n    visualize_training_history(all_results_v1)\n\n# 2. Test metrics comparison\nif all_test_metrics:\n    visualize_test_metrics_comparison(all_test_metrics)\n\n# 3. Sample predictions\nif all_test_metrics:\n    visualize_sample_predictions(all_test_metrics, num_samples=3)\n\n# 4. Confusion matrices\nif all_test_metrics:\n    create_confusion_matrix_heatmap(all_test_metrics)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ ALL VISUALIZATIONS COMPLETED!\")\nprint(\"=\"*60)\nprint(f\"üìÅ Visualizations saved to: {vis_dir}\")\nprint(\"\\nüìã Generated files:\")\nfor file in os.listdir(vis_dir):\n    size = os.path.getsize(os.path.join(vis_dir, file)) / 1024\n    print(f\"  ‚Ä¢ {file} ({size:.1f} KB)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:27:50.983322Z","iopub.execute_input":"2025-12-08T21:27:50.983950Z","iopub.status.idle":"2025-12-08T21:27:51.254233Z","shell.execute_reply.started":"2025-12-08T21:27:50.983918Z","shell.execute_reply":"2025-12-08T21:27:51.253566Z"}},"outputs":[],"execution_count":null}]}