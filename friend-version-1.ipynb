{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# CELL 1: SETUP & INSTALL (Kaggle Version 2)\n# ============================================\nimport os\nimport sys\n\n# Install required packages\n!pip install roboflow -q\n!pip install segmentation-models-pytorch -q\n!pip install albumentations -q\n!pip install gradio -q\n!pip install pycocotools -q\n!pip install torchmetrics -q\n!pip install pandas tabulate -q\n\nprint(\"âœ… Kaggle setup complete for Version 2!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:32:56.430955Z","iopub.execute_input":"2025-12-08T21:32:56.431267Z","iopub.status.idle":"2025-12-08T21:35:09.591382Z","shell.execute_reply.started":"2025-12-08T21:32:56.431233Z","shell.execute_reply":"2025-12-08T21:35:09.590012Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ns3fs 2025.3.0 requires fsspec==2025.3.0.*, but you have fsspec 2025.10.0 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\ngoogle-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\njupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mâœ… Kaggle setup complete for Version 2!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ============================================\n# CELL 2: DOWNLOAD DATASET (Version 2)\n# ============================================\nfrom roboflow import Roboflow\n\n# Your API key\nrf = Roboflow(api_key=\"FoHdZwbhLlvtF4Xo4zdZ\")\nproject = rf.workspace(\"studentdatasets\").project(\"microscopy-cell-segmentation\")\nversion = project.version(21)\ndataset = version.download(\"coco-segmentation\")\n\nprint(\"âœ… Dataset downloaded for Version 2!\")\ndataset_path = dataset.location","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:35:14.047478Z","iopub.execute_input":"2025-12-08T21:35:14.047919Z","iopub.status.idle":"2025-12-08T21:35:39.636668Z","shell.execute_reply.started":"2025-12-08T21:35:14.047866Z","shell.execute_reply":"2025-12-08T21:35:39.635401Z"}},"outputs":[{"name":"stdout","text":"loading Roboflow workspace...\nloading Roboflow project...\n","output_type":"stream"},{"name":"stderr","text":"Downloading Dataset Version Zip in Microscopy-Cell-Segmentation-21 to coco-segmentation:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 411266/411266 [00:18<00:00, 21913.34it/s]","output_type":"stream"},{"name":"stdout","text":"\n","output_type":"stream"},{"name":"stderr","text":"\nExtracting Dataset Version Zip to Microscopy-Cell-Segmentation-21 in coco-segmentation:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7041/7041 [00:01<00:00, 4394.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"âœ… Dataset downloaded for Version 2!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ============================================\n# CELL 3: IMPORTS & GPU SETUP (Version 2)\n# ============================================\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport json\nimport os\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport segmentation_models_pytorch as smp\nimport pandas as pd\nimport torch.nn.functional as F\nfrom torchmetrics.classification import BinaryJaccardIndex, BinaryF1Score\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n\n# Check GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"ğŸš€ Using device: {device} (Version 2)\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n    \n# Create output directory\noutput_dir = '/kaggle/working/cell_segmentation_v2'\nos.makedirs(output_dir, exist_ok=True)\nprint(f\"ğŸ“ Output directory: {output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:35:42.992581Z","iopub.execute_input":"2025-12-08T21:35:42.992929Z","iopub.status.idle":"2025-12-08T21:36:04.083102Z","shell.execute_reply.started":"2025-12-08T21:35:42.992904Z","shell.execute_reply":"2025-12-08T21:36:04.082030Z"}},"outputs":[{"name":"stdout","text":"ğŸš€ Using device: cpu (Version 2)\nğŸ“ Output directory: /kaggle/working/cell_segmentation_v2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ============================================\n# CELL 4: IMPROVED DATASET CLASS (Version 2)\n# ============================================\nclass CellSegmentationDataset(Dataset):\n    \"\"\"Improved dataset with better augmentation\"\"\"\n    def __init__(self, json_path, img_dir, img_size=512, augment=True):\n        with open(json_path) as f:\n            data = json.load(f)\n        \n        self.images = data['images']\n        self.annotations = data['annotations']\n        self.img_dir = img_dir\n        self.img_size = img_size\n        self.augment = augment\n        \n        # Create annotation mapping\n        self.ann_map = {}\n        for ann in self.annotations:\n            img_id = ann['image_id']\n            if img_id not in self.ann_map:\n                self.ann_map[img_id] = []\n            self.ann_map[img_id].append(ann)\n        \n        self.image_paths = [os.path.join(img_dir, img['file_name']) for img in self.images]\n        \n        # Enhanced augmentations for microscopy\n        if augment:\n            self.transform = A.Compose([\n                A.Resize(img_size, img_size, always_apply=True),\n                A.HorizontalFlip(p=0.5),\n                A.VerticalFlip(p=0.5),\n                A.RandomRotate90(p=0.5),\n                A.RandomBrightnessContrast(p=0.3, brightness_limit=0.1, contrast_limit=0.1),\n                A.GaussianBlur(p=0.1, blur_limit=(3, 7)),\n                A.GaussNoise(p=0.1, var_limit=(10.0, 50.0)),\n                A.ElasticTransform(p=0.2, alpha=1, sigma=50, alpha_affine=50),\n                A.CoarseDropout(p=0.1, max_holes=8, max_height=32, max_width=32, fill_value=0),\n                A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n                ToTensorV2(),\n            ])\n        else:\n            self.transform = A.Compose([\n                A.Resize(img_size, img_size, always_apply=True),\n                A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n                ToTensorV2(),\n            ])\n    \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img_info = self.images[idx]\n        \n        # Create mask\n        mask = np.zeros((self.img_size, self.img_size), dtype=np.float32)\n        \n        if img_info['id'] in self.ann_map:\n            for ann in self.ann_map[img_info['id']]:\n                for seg in ann['segmentation']:\n                    pts = np.array(seg).reshape(-1, 2)\n                    if len(pts) > 0:\n                        # Preserve aspect ratio\n                        pts[:, 0] = pts[:, 0] * self.img_size / img_info['width']\n                        pts[:, 1] = pts[:, 1] * self.img_size / img_info['height']\n                        pts = pts.astype(np.int32)\n                        cv2.fillPoly(mask, [pts], 1)\n        \n        transformed = self.transform(image=img, mask=mask)\n        img_tensor = transformed['image']\n        mask_tensor = transformed['mask']\n        \n        return img_tensor, mask_tensor.float()\n\n# Create datasets\nprint(\"ğŸ“Š Creating datasets for Version 2...\")\ntrain_dataset = CellSegmentationDataset(\n    os.path.join(dataset_path, \"train\", \"_annotations.coco.json\"),\n    os.path.join(dataset_path, \"train\"),\n    augment=True\n)\n\nval_dataset = CellSegmentationDataset(\n    os.path.join(dataset_path, \"valid\", \"_annotations.coco.json\"),\n    os.path.join(dataset_path, \"valid\"),\n    augment=False\n)\n\ntest_dataset = CellSegmentationDataset(\n    os.path.join(dataset_path, \"test\", \"_annotations.coco.json\"),\n    os.path.join(dataset_path, \"test\"),\n    augment=False\n)\n\nprint(f\"âœ… Datasets created for Version 2!\")\nprint(f\"Train: {len(train_dataset)} images\")\nprint(f\"Validation: {len(val_dataset)} images\")\nprint(f\"Test: {len(test_dataset)} images\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:36:09.439581Z","iopub.execute_input":"2025-12-08T21:36:09.440211Z","iopub.status.idle":"2025-12-08T21:36:10.756206Z","shell.execute_reply.started":"2025-12-08T21:36:09.440182Z","shell.execute_reply":"2025-12-08T21:36:10.755333Z"}},"outputs":[{"name":"stdout","text":"ğŸ“Š Creating datasets for Version 2...\nâœ… Datasets created for Version 2!\nTrain: 4950 images\nValidation: 1398 images\nTest: 685 images\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ============================================\n# CELL 5: CREATE 2 MODELS (Version 2 - Models 3 & 4)\n# ============================================\nprint(\"ğŸ§  CREATING 2 MODELS FOR VERSION 2 (Friend 1)...\")\nprint(\"=\"*50)\n\n# 3. FPN with EfficientNet-B3\nprint(\"1. Creating FPN EfficientNet-B3...\")\nmodel3 = smp.FPN(\n    encoder_name=\"timm-efficientnet-b3\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=1,\n    activation=None,\n    decoder_dropout=0.2\n).to(device)\n\n# 4. MA-Net (Medical Attention Network)\nprint(\"2. Creating MA-Net ResNet34...\")\nmodel4 = smp.MAnet(\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=1,\n    activation=None\n).to(device)\n\nmodels_v2 = {\n    'fpn_effb3': model3,\n    'manet_r34': model4\n}\n\nfor name, model in models_v2.items():\n    params = sum(p.numel() for p in model.parameters()) / 1e6\n    print(f\"{name}: {params:.1f}M parameters\")\nprint(\"âœ… 2 Models created for Version 2 (Friend 1)\")\nprint(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:36:15.188554Z","iopub.execute_input":"2025-12-08T21:36:15.189543Z","iopub.status.idle":"2025-12-08T21:36:26.399124Z","shell.execute_reply.started":"2025-12-08T21:36:15.189501Z","shell.execute_reply":"2025-12-08T21:36:26.398012Z"}},"outputs":[{"name":"stdout","text":"ğŸ§  CREATING 2 MODELS FOR VERSION 2 (Friend 1)...\n==================================================\n1. Creating FPN EfficientNet-B3...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/106 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc141bcdf9fd4de2b58fdcfe630abd2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/49.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18cce71e22dc4fef9dd7df595f60c84c"}},"metadata":{}},{"name":"stdout","text":"2. Creating MA-Net ResNet34...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3ef3668e15c4ce8b5d7a86ca0fee527"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/87.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba47a0c26171453b91c497618118c413"}},"metadata":{}},{"name":"stdout","text":"fpn_effb3: 12.5M parameters\nmanet_r34: 31.8M parameters\nâœ… 2 Models created for Version 2 (Friend 1)\n==================================================\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ============================================\n# CELL 6: ENHANCED TRAINER WITH COMPREHENSIVE METRICS (Version 2)\n# ============================================\nclass EnhancedTrainerV2:\n    def __init__(self, device='cuda'):\n        self.device = device\n        self.bce_loss = nn.BCEWithLogitsLoss()\n        self.dice_loss = smp.losses.DiceLoss(mode='binary')\n        self.focal_loss = smp.losses.FocalLoss(mode='binary')\n        \n        # Comprehensive metrics\n        self.iou_metric = BinaryJaccardIndex().to(device)\n        self.f1_metric = BinaryF1Score().to(device)\n    \n    def create_dataloaders(self, batch_size=8):\n        \"\"\"Create dataloaders\"\"\"\n        train_loader = DataLoader(\n            train_dataset, \n            batch_size=batch_size, \n            shuffle=True,\n            num_workers=2,\n            pin_memory=True\n        )\n        \n        val_loader = DataLoader(\n            val_dataset, \n            batch_size=batch_size, \n            shuffle=False,\n            num_workers=2,\n            pin_memory=True\n        )\n        \n        test_loader = DataLoader(\n            test_dataset, \n            batch_size=batch_size, \n            shuffle=False,\n            num_workers=2,\n            pin_memory=True\n        )\n        \n        return train_loader, val_loader, test_loader\n    \n    def calculate_comprehensive_metrics(self, outputs, targets, threshold=0.5):\n        \"\"\"Calculate all metrics for segmentation\"\"\"\n        with torch.no_grad():\n            preds = torch.sigmoid(outputs)\n            preds_binary = (preds > threshold).float()\n            \n            # Basic metrics\n            iou = self.iou_metric(preds_binary, targets)\n            f1 = self.f1_metric(preds_binary, targets)\n            \n            # Additional metrics\n            intersection = (preds_binary * targets).sum()\n            union = preds_binary.sum() + targets.sum()\n            dice = (2 * intersection) / (union + 1e-7)\n            \n            tp = (preds_binary * targets).sum()\n            fp = (preds_binary * (1 - targets)).sum()\n            fn = ((1 - preds_binary) * targets).sum()\n            tn = ((1 - preds_binary) * (1 - targets)).sum()\n            \n            precision = tp / (tp + fp + 1e-7)\n            recall = tp / (tp + fn + 1e-7)\n            specificity = tn / (tn + fp + 1e-7)\n            accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-7)\n            \n            return {\n                'iou': iou.item(),\n                'f1': f1.item(),\n                'dice': dice.item(),\n                'precision': precision.item(),\n                'recall': recall.item(),\n                'specificity': specificity.item(),\n                'accuracy': accuracy.item(),\n                'tp': tp.item(),\n                'fp': fp.item(),\n                'fn': fn.item(),\n                'tn': tn.item()\n            }\n    \n    def combined_loss(self, outputs, targets):\n        \"\"\"Weighted combination of multiple losses\"\"\"\n        bce = self.bce_loss(outputs, targets)\n        dice = self.dice_loss(outputs, targets)\n        focal = self.focal_loss(outputs, targets)\n        return 0.4*bce + 0.4*dice + 0.2*focal\n    \n    def train_epoch(self, model, loader, optimizer, scaler=None, epoch=None):\n        \"\"\"Train for one epoch with comprehensive metrics\"\"\"\n        model.train()\n        epoch_metrics = {\n            'loss': 0, 'iou': 0, 'f1': 0, 'dice': 0,\n            'precision': 0, 'recall': 0, 'accuracy': 0\n        }\n        \n        pbar = tqdm(loader, desc=f'Training Epoch {epoch+1}')\n        for images, masks in pbar:\n            images, masks = images.to(self.device), masks.to(self.device).unsqueeze(1)\n            \n            optimizer.zero_grad()\n            \n            if scaler is not None:\n                with torch.cuda.amp.autocast():\n                    outputs = model(images)\n                    loss = self.combined_loss(outputs, masks)\n                \n                scaler.scale(loss).backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                outputs = model(images)\n                loss = self.combined_loss(outputs, masks)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                optimizer.step()\n            \n            # Calculate metrics\n            metrics = self.calculate_comprehensive_metrics(outputs, masks)\n            \n            # Update epoch metrics\n            epoch_metrics['loss'] += loss.item()\n            for key in metrics:\n                if key in epoch_metrics:\n                    epoch_metrics[key] += metrics[key]\n            \n            # Update progress bar\n            pbar.set_postfix({\n                'loss': f\"{loss.item():.4f}\",\n                'iou': f\"{metrics['iou']:.4f}\",\n                'dice': f\"{metrics['dice']:.4f}\"\n            })\n        \n        # Average metrics\n        for key in epoch_metrics:\n            epoch_metrics[key] /= len(loader)\n        \n        return epoch_metrics\n    \n    def validate(self, model, loader, split='Validation'):\n        \"\"\"Validate model with comprehensive metrics\"\"\"\n        model.eval()\n        val_metrics = {\n            'loss': 0, 'iou': 0, 'f1': 0, 'dice': 0,\n            'precision': 0, 'recall': 0, 'accuracy': 0\n        }\n        \n        with torch.no_grad():\n            for images, masks in tqdm(loader, desc=split):\n                images, masks = images.to(self.device), masks.to(self.device).unsqueeze(1)\n                outputs = model(images)\n                \n                loss = self.combined_loss(outputs, masks)\n                metrics = self.calculate_comprehensive_metrics(outputs, masks)\n                \n                val_metrics['loss'] += loss.item()\n                for key in metrics:\n                    if key in val_metrics:\n                        val_metrics[key] += metrics[key]\n        \n        # Average metrics\n        for key in val_metrics:\n            val_metrics[key] /= len(loader)\n        \n        return val_metrics\n    \n    def train_model(self, model, train_loader, val_loader, model_name, \n                   epochs=30, lr=1e-4, patience=10):\n        \"\"\"Complete training with comprehensive tracking\"\"\"\n        print(f\"\\nğŸš€ Training {model_name} for {epochs} epochs...\")\n        print(\"=\"*60)\n        \n        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode='max', factor=0.5, patience=5, verbose=True, min_lr=1e-6\n        )\n        \n        scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n        \n        # Comprehensive history\n        history = {\n            'train': {k: [] for k in ['loss', 'iou', 'f1', 'dice', 'precision', 'recall', 'accuracy']},\n            'val': {k: [] for k in ['loss', 'iou', 'f1', 'dice', 'precision', 'recall', 'accuracy']},\n            'lr': [],\n            'best_epoch': 0\n        }\n        \n        best_iou = 0\n        patience_counter = 0\n        best_model_state = None\n        \n        for epoch in range(epochs):\n            print(f\"\\n{'='*60}\")\n            print(f\"Epoch {epoch+1}/{epochs}\")\n            print('='*60)\n            \n            # Training\n            train_metrics = self.train_epoch(model, train_loader, optimizer, scaler, epoch)\n            for key in train_metrics:\n                history['train'][key].append(train_metrics[key])\n            \n            # Validation\n            val_metrics = self.validate(model, val_loader, 'Validation')\n            for key in val_metrics:\n                history['val'][key].append(val_metrics[key])\n            \n            # Learning rate tracking\n            current_lr = optimizer.param_groups[0]['lr']\n            history['lr'].append(current_lr)\n            \n            # Print epoch results\n            print(f\"Train - Loss: {train_metrics['loss']:.4f}, IoU: {train_metrics['iou']:.4f}, Dice: {train_metrics['dice']:.4f}\")\n            print(f\"Val   - Loss: {val_metrics['loss']:.4f}, IoU: {val_metrics['iou']:.4f}, Dice: {val_metrics['dice']:.4f}\")\n            print(f\"Metrics - Precision: {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}, Accuracy: {val_metrics['accuracy']:.4f}\")\n            print(f\"Learning Rate: {current_lr:.6f}\")\n            \n            # Update scheduler\n            scheduler.step(val_metrics['iou'])\n            \n            # Early stopping and model saving\n            if val_metrics['iou'] > best_iou:\n                best_iou = val_metrics['iou']\n                patience_counter = 0\n                history['best_epoch'] = epoch\n                best_model_state = model.state_dict().copy()\n                \n                # Save best model\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'val_iou': best_iou,\n                    'history': history,\n                    'val_metrics': val_metrics\n                }, os.path.join(output_dir, f'{model_name}_best.pth'))\n                print(f\"ğŸ’¾ Saved best model with IoU: {best_iou:.4f}\")\n            else:\n                patience_counter += 1\n                print(f\"â³ No improvement ({patience_counter}/{patience})\")\n            \n            if patience_counter >= patience:\n                print(f\"â¹ï¸ Early stopping at epoch {epoch+1}\")\n                break\n        \n        # Restore best model\n        if best_model_state is not None:\n            model.load_state_dict(best_model_state)\n        \n        # Save final model and history\n        torch.save({\n            'model_state_dict': model.state_dict(),\n            'history': history,\n            'best_iou': best_iou,\n            'final_epoch': epoch\n        }, os.path.join(output_dir, f'{model_name}_final.pth'))\n        \n        # Save metrics to CSV\n        metrics_df = pd.DataFrame({\n            'epoch': list(range(1, len(history['train']['loss']) + 1)),\n            'train_loss': history['train']['loss'],\n            'val_loss': history['val']['loss'],\n            'train_iou': history['train']['iou'],\n            'val_iou': history['val']['iou'],\n            'train_dice': history['train']['dice'],\n            'val_dice': history['val']['dice'],\n            'val_precision': history['val']['precision'],\n            'val_recall': history['val']['recall'],\n            'val_accuracy': history['val']['accuracy'],\n            'learning_rate': history['lr']\n        })\n        metrics_df.to_csv(os.path.join(output_dir, f'{model_name}_metrics.csv'), index=False)\n        \n        print(f\"\\nâœ… Training completed for {model_name}!\")\n        print(f\"ğŸ“Š Best Validation IoU: {best_iou:.4f} at epoch {history['best_epoch'] + 1}\")\n        print(f\"ğŸ’¾ Models saved to: {output_dir}\")\n        \n        return history, best_iou\n\n# Initialize trainer\ntrainer_v2 = EnhancedTrainerV2(device=device)\nprint(\"âœ… Enhanced trainer created for Version 2!\")\n\n# Create dataloaders\ntrain_loader, val_loader, test_loader = trainer_v2.create_dataloaders(batch_size=8)\nprint(f\"ğŸ“Š Dataloaders created for Version 2:\")\nprint(f\"   Train batches: {len(train_loader)}\")\nprint(f\"   Val batches: {len(val_loader)}\")\nprint(f\"   Test batches: {len(test_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:36:29.460709Z","iopub.execute_input":"2025-12-08T21:36:29.461042Z","iopub.status.idle":"2025-12-08T21:36:29.504229Z","shell.execute_reply.started":"2025-12-08T21:36:29.461018Z","shell.execute_reply":"2025-12-08T21:36:29.502996Z"}},"outputs":[{"name":"stdout","text":"âœ… Enhanced trainer created for Version 2!\nğŸ“Š Dataloaders created for Version 2:\n   Train batches: 619\n   Val batches: 175\n   Test batches: 86\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ============================================\n# CELL 7a: TRAIN MODEL 3 - FPN EfficientNet-B3 (30 EPOCHS)\n# ============================================\nprint(\"=\"*60)\nprint(\"1. TRAINING: FPN EfficientNet-B3 (30 EPOCHS)\")\nprint(\"=\"*60)\n\n# Initialize results storage if not exists\nif 'all_results_v2' not in globals():\n    all_results_v2 = {}\n\n# Train Model 3\nhistory3, best_iou3 = trainer_v2.train_model(\n    model=model3,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    model_name=\"fpn_effb3\",\n    epochs=30,  # Increased to 30\n    lr=1e-4,\n    patience=10\n)\n\nall_results_v2['fpn_effb3'] = {\n    'history': history3,\n    'best_iou': best_iou3,\n    'model': model3,\n    'params': f\"{sum(p.numel() for p in model3.parameters()) / 1e6:.1f}M\"\n}\n\nprint(f\"\\nâœ… Model 3 Training Completed!\")\nprint(f\"ğŸ“Š Best Validation IoU: {best_iou3:.4f}\")\nprint(f\"ğŸ”¢ Parameters: {all_results_v2['fpn_effb3']['params']}\")\n\n# Save individual model checkpoint\ntorch.save({\n    'model_state_dict': model3.state_dict(),\n    'best_iou': best_iou3,\n    'history': history3,\n    'epochs': 30\n}, os.path.join(output_dir, 'model3_complete.pth'))\n\nprint(f\"ğŸ’¾ Model 3 saved to: {output_dir}/model3_complete.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:36:38.901730Z","iopub.execute_input":"2025-12-08T21:36:38.902049Z","iopub.status.idle":"2025-12-08T21:37:31.241645Z","shell.execute_reply.started":"2025-12-08T21:36:38.902027Z","shell.execute_reply":"2025-12-08T21:37:31.240062Z"}},"outputs":[{"name":"stdout","text":"============================================================\n1. TRAINING: FPN EfficientNet-B3 (30 EPOCHS)\n============================================================\n\nğŸš€ Training fpn_effb3 for 30 epochs...\n============================================================\n\n============================================================\nEpoch 1/30\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1:   0%|          | 1/619 [00:52<8:56:32, 52.09s/it, loss=1.1576, iou=0.1104, dice=0.1989]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2453850734.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Train Model 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m history3, best_iou3 = trainer_v2.train_model(\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/3309136613.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, model, train_loader, val_loader, model_name, epochs, lr, patience)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/3309136613.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, model, loader, optimizer, scaler, epoch)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombined_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"# ============================================\n# CELL 7b: TRAIN MODEL 4 - MA-Net ResNet34 (30 EPOCHS)\n# ============================================\nprint(\"=\"*60)\nprint(\"2. TRAINING: MA-Net ResNet34 (30 EPOCHS)\")\nprint(\"=\"*60)\n\n# Train Model 4\nhistory4, best_iou4 = trainer_v2.train_model(\n    model=model4,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    model_name=\"manet_r34\",\n    epochs=30,  # Increased to 30\n    lr=5e-5,\n    patience=10\n)\n\nall_results_v2['manet_r34'] = {\n    'history': history4,\n    'best_iou': best_iou4,\n    'model': model4,\n    'params': f\"{sum(p.numel() for p in model4.parameters()) / 1e6:.1f}M\"\n}\n\nprint(f\"\\nâœ… Model 4 Training Completed!\")\nprint(f\"ğŸ“Š Best Validation IoU: {best_iou4:.4f}\")\nprint(f\"ğŸ”¢ Parameters: {all_results_v2['manet_r34']['params']}\")\n\n# Save individual model checkpoint\ntorch.save({\n    'model_state_dict': model4.state_dict(),\n    'best_iou': best_iou4,\n    'history': history4,\n    'epochs': 30\n}, os.path.join(output_dir, 'model4_complete.pth'))\n\nprint(f\"ğŸ’¾ Model 4 saved to: {output_dir}/model4_complete.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:37:34.708873Z","iopub.execute_input":"2025-12-08T21:37:34.709272Z","iopub.status.idle":"2025-12-08T21:38:09.079782Z","shell.execute_reply.started":"2025-12-08T21:37:34.709239Z","shell.execute_reply":"2025-12-08T21:38:09.078194Z"}},"outputs":[{"name":"stdout","text":"============================================================\n2. TRAINING: MA-Net ResNet34 (30 EPOCHS)\n============================================================\n\nğŸš€ Training manet_r34 for 30 epochs...\n============================================================\n\n============================================================\nEpoch 1/30\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1:   0%|          | 1/619 [00:34<5:53:02, 34.28s/it, loss=0.7347, iou=0.0181, dice=0.0355]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2165481055.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Train Model 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m history4, best_iou4 = trainer_v2.train_model(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/3309136613.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, model, train_loader, val_loader, model_name, epochs, lr, patience)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/3309136613.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, model, loader, optimizer, scaler, epoch)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombined_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/segmentation_models_pytorch/base/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegmentation_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/segmentation_models_pytorch/decoders/manet/decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mskips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_block\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskips\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskips\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/segmentation_models_pytorch/decoders/manet/decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msp_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"# ============================================\n# CELL 7c: VERSION 2 TRAINING SUMMARY\n# ============================================\nprint(\"=\"*60)\nprint(\"âœ… VERSION 2 TRAINING COMPLETED! (Friend 1)\")\nprint(\"=\"*60)\n\nprint(\"\\nğŸ“Š TRAINING SUMMARY:\")\nprint(\"-\" * 40)\nfor name, data in all_results_v2.items():\n    print(f\"Model: {name}\")\n    print(f\"  â€¢ Best Validation IoU: {data['best_iou']:.4f}\")\n    print(f\"  â€¢ Parameters: {data['params']}\")\n    \n    # Show some training history\n    if 'history' in data and 'val' in data['history']:\n        val_history = data['history']['val']\n        if 'iou' in val_history and len(val_history['iou']) > 0:\n            print(f\"  â€¢ Final Epoch IoU: {val_history['iou'][-1]:.4f}\")\n            print(f\"  â€¢ Best Epoch: {data['history']['best_epoch'] + 1}\")\n    print(\"-\" * 40)\n\n# Save comprehensive results summary\nimport json\nsummary_v2 = {\n    'version': 'VERSION_2_MODELS_3_4',\n    'models_trained': list(all_results_v2.keys()),\n    'training_details': {},\n    'timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n    'epochs': 30,\n    'device': str(device),\n    'friend': 'Friend 1'\n}\n\nfor name, data in all_results_v2.items():\n    summary_v2['training_details'][name] = {\n        'best_iou': float(data['best_iou']),\n        'params': data['params'],\n        'final_epoch': len(data['history']['train']['loss']) if 'history' in data else 0\n    }\n\nsummary_path = os.path.join(output_dir, 'version_2_summary.json')\nwith open(summary_path, 'w') as f:\n    json.dump(summary_v2, f, indent=2)\n\nprint(f\"\\nğŸ’¾ Version 2 summary saved to: {summary_path}\")\nprint(f\"ğŸ“ Output directory: {output_dir}\")\n\n# List all saved files\nprint(\"\\nğŸ“‹ Saved files in output directory:\")\nfor file in os.listdir(output_dir):\n    if file.endswith('.pth') or file.endswith('.json') or file.endswith('.csv'):\n        size = os.path.getsize(os.path.join(output_dir, file)) / 1024\n        print(f\"  â€¢ {file} ({size:.1f} KB)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# CELL 8: COMPREHENSIVE TEST EVALUATION (Version 2)\n# ============================================\nprint(\"=\"*60)\nprint(\"ğŸ“Š COMPREHENSIVE TEST SET EVALUATION (Version 2)\")\nprint(\"=\"*60)\n\ndef evaluate_model_test_v2(model, test_loader, model_name, trainer):\n    \"\"\"Comprehensive test evaluation for Version 2\"\"\"\n    model.eval()\n    \n    metrics_sum = {\n        'loss': 0, 'iou': 0, 'f1': 0, 'dice': 0,\n        'precision': 0, 'recall': 0, 'accuracy': 0,\n        'specificity': 0\n    }\n    \n    batch_metrics = {k: [] for k in metrics_sum.keys()}\n    \n    with torch.no_grad():\n        for batch_idx, (images, masks) in enumerate(tqdm(test_loader, desc=f'Testing {model_name}')):\n            images, masks = images.to(device), masks.to(device).unsqueeze(1)\n            outputs = model(images)\n            \n            loss = trainer.combined_loss(outputs, masks)\n            metrics = trainer.calculate_comprehensive_metrics(outputs, masks)\n            \n            metrics_sum['loss'] += loss.item()\n            for key in metrics:\n                if key in metrics_sum:\n                    metrics_sum[key] += metrics[key]\n                    batch_metrics[key].append(metrics[key])\n    \n    # Calculate means and standard deviations\n    num_batches = len(test_loader)\n    results_mean = {}\n    results_std = {}\n    \n    for key in metrics_sum:\n        results_mean[key] = metrics_sum[key] / num_batches\n        if batch_metrics[key]:\n            results_std[key] = np.std(batch_metrics[key])\n        else:\n            results_std[key] = 0\n    \n    return results_mean, results_std\n\n# Evaluate both models\nprint(\"\\nğŸ” Evaluating models on test set...\")\ntest_results_v2 = []\nall_test_metrics_v2 = {}\n\nfor model_name, data in all_results_v2.items():\n    print(f\"\\n{'='*40}\")\n    print(f\"Evaluating {model_name}...\")\n    print('='*40)\n    \n    model = data['model']\n    mean_metrics, std_metrics = evaluate_model_test_v2(model, test_loader, model_name, trainer_v2)\n    \n    # Store results\n    test_results_v2.append({\n        'Model': model_name,\n        'Test_IoU_Mean': f\"{mean_metrics['iou']:.4f}\",\n        'Test_IoU_Std': f\"{std_metrics['iou']:.4f}\",\n        'Test_Dice_Mean': f\"{mean_metrics['dice']:.4f}\",\n        'Test_Dice_Std': f\"{std_metrics['dice']:.4f}\",\n        'Test_F1_Mean': f\"{mean_metrics['f1']:.4f}\",\n        'Test_F1_Std': f\"{std_metrics['f1']:.4f}\",\n        'Precision': f\"{mean_metrics['precision']:.4f}\",\n        'Recall': f\"{mean_metrics['recall']:.4f}\",\n        'Accuracy': f\"{mean_metrics['accuracy']:.4f}\",\n        'Specificity': f\"{mean_metrics['specificity']:.4f}\",\n        'Val_IoU_Best': f\"{data['best_iou']:.4f}\",\n        'Parameters': data['params']\n    })\n    \n    # Store detailed metrics\n    all_test_metrics_v2[model_name] = {\n        'mean': mean_metrics,\n        'std': std_metrics\n    }\n    \n    # Print detailed metrics\n    print(f\"\\nğŸ“Š Test Metrics for {model_name}:\")\n    print(f\"  â€¢ IoU:        {mean_metrics['iou']:.4f} Â± {std_metrics['iou']:.4f}\")\n    print(f\"  â€¢ Dice:       {mean_metrics['dice']:.4f} Â± {std_metrics['dice']:.4f}\")\n    print(f\"  â€¢ F1-Score:   {mean_metrics['f1']:.4f} Â± {std_metrics['f1']:.4f}\")\n    print(f\"  â€¢ Precision:  {mean_metrics['precision']:.4f}\")\n    print(f\"  â€¢ Recall:     {mean_metrics['recall']:.4f}\")\n    print(f\"  â€¢ Accuracy:   {mean_metrics['accuracy']:.4f}\")\n    print(f\"  â€¢ Specificity: {mean_metrics['specificity']:.4f}\")\n\n# Display results as table\nprint(\"\\n\" + \"=\"*80)\nprint(\"ğŸ† FINAL TEST EVALUATION RESULTS (Version 2)\")\nprint(\"=\"*80)\n\nif test_results_v2:\n    df_results = pd.DataFrame(test_results_v2)\n    from tabulate import tabulate\n    print(tabulate(df_results, headers='keys', tablefmt='pretty', showindex=False))\n    \n    # Save results to CSV\n    results_csv_path = os.path.join(output_dir, 'test_evaluation_results.csv')\n    df_results.to_csv(results_csv_path, index=False)\n    print(f\"\\nğŸ’¾ Results saved to: {results_csv_path}\")\nelse:\n    print(\"âš ï¸ No evaluation results to display\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# CELL 9: COMPREHENSIVE VISUALIZATION (Version 2)\n# ============================================\nprint(\"=\"*60)\nprint(\"ğŸ“Š COMPREHENSIVE VISUALIZATION (Version 2)\")\nprint(\"=\"*60)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import gridspec\n\n# Set style\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\n# Create visualization directory\nvis_dir = os.path.join(output_dir, 'visualizations')\nos.makedirs(vis_dir, exist_ok=True)\n\ndef visualize_training_history_v2(all_results_dict):\n    \"\"\"Visualize training history for Version 2 models\"\"\"\n    fig = plt.figure(figsize=(18, 12))\n    gs = gridspec.GridSpec(3, 3, figure=fig)\n    \n    metrics_to_plot = ['loss', 'iou', 'dice', 'precision', 'recall', 'accuracy']\n    titles = ['Loss', 'IoU', 'Dice Coefficient', 'Precision', 'Recall', 'Accuracy']\n    \n    for idx, (metric, title) in enumerate(zip(metrics_to_plot, titles)):\n        ax = fig.add_subplot(gs[idx // 3, idx % 3])\n        \n        for model_name, data in all_results_dict.items():\n            if 'history' in data and 'train' in data['history']:\n                train_vals = data['history']['train'].get(metric, [])\n                val_vals = data['history']['val'].get(metric, [])\n                \n                if train_vals and val_vals:\n                    epochs = range(1, len(train_vals) + 1)\n                    ax.plot(epochs, train_vals, '--', linewidth=1.5, label=f'{model_name} Train')\n                    ax.plot(epochs, val_vals, '-', linewidth=2, label=f'{model_name} Val')\n        \n        ax.set_xlabel('Epoch')\n        ax.set_ylabel(title)\n        ax.set_title(f'Training vs Validation {title}')\n        ax.legend(fontsize=8)\n        ax.grid(True, alpha=0.3)\n    \n    # Learning rate plot\n    ax_lr = fig.add_subplot(gs[2, :])\n    for model_name, data in all_results_dict.items():\n        if 'history' in data and 'lr' in data['history']:\n            lr_vals = data['history']['lr']\n            if lr_vals:\n                epochs = range(1, len(lr_vals) + 1)\n                ax_lr.plot(epochs, lr_vals, 'o-', linewidth=2, label=model_name)\n    \n    ax_lr.set_xlabel('Epoch')\n    ax_lr.set_ylabel('Learning Rate')\n    ax_lr.set_title('Learning Rate Schedule')\n    ax_lr.set_yscale('log')\n    ax_lr.legend()\n    ax_lr.grid(True, alpha=0.3)\n    \n    plt.suptitle('Version 2: Training History Comparison', fontsize=16, fontweight='bold', y=1.02)\n    plt.tight_layout()\n    \n    save_path = os.path.join(vis_dir, 'training_history_comparison.png')\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    plt.show()\n    print(f\"âœ… Training history saved to: {save_path}\")\n\ndef visualize_sample_predictions_v2(test_loader, models_dict, num_samples=3):\n    \"\"\"Visualize sample predictions for Version 2\"\"\"\n    fig, axes = plt.subplots(num_samples, len(models_dict) + 2, figsize=(20, 4*num_samples))\n    \n    # Get sample data\n    sample_data = []\n    for i, (images, masks) in enumerate(test_loader):\n        if i >= num_samples:\n            break\n        sample_data.append((images[0], masks[0]))\n    \n    for sample_idx in range(num_samples):\n        img, true_mask = sample_data[sample_idx]\n        img_np = img.numpy().transpose(1, 2, 0)\n        img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n        img_np = np.clip(img_np, 0, 1)\n        \n        true_mask_np = true_mask.numpy()\n        \n        # Original image\n        axes[sample_idx, 0].imshow(img_np)\n        axes[sample_idx, 0].set_title('Original Image', fontsize=10, fontweight='bold')\n        axes[sample_idx, 0].axis('off')\n        \n        # Ground truth\n        axes[sample_idx, 1].imshow(true_mask_np, cmap='gray')\n        axes[sample_idx, 1].set_title('Ground Truth', fontsize=10, fontweight='bold')\n        axes[sample_idx, 1].axis('off')\n        \n        # Each model's prediction\n        for model_idx, (model_name, model_data) in enumerate(models_dict.items()):\n            model = model_data['model']\n            model.eval()\n            \n            with torch.no_grad():\n                img_tensor = img.unsqueeze(0).to(device)\n                output = model(img_tensor)\n                pred = torch.sigmoid(output).squeeze().cpu().numpy()\n                pred_binary = (pred > 0.5).astype(np.float32)\n            \n            # Calculate Dice\n            intersection = (pred_binary * true_mask_np).sum()\n            union = pred_binary.sum() + true_mask_np.sum()\n            dice = (2 * intersection) / (union + 1e-7)\n            \n            # Display prediction\n            ax = axes[sample_idx, model_idx + 2]\n            ax.imshow(pred_binary, cmap='gray')\n            ax.set_title(f'{model_name}\\nDice: {dice:.3f}', fontsize=9)\n            ax.axis('off')\n    \n    plt.suptitle('Version 2: Model Predictions Comparison', fontsize=16, fontweight='bold', y=1.02)\n    plt.tight_layout()\n    \n    save_path = os.path.join(vis_dir, 'sample_predictions.png')\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    plt.show()\n    print(f\"âœ… Sample predictions saved to: {save_path}\")\n\n# Run visualizations\nprint(\"\\nğŸ“ˆ Generating visualizations for Version 2...\")\n\n# 1. Training history\nif all_results_v2:\n    visualize_training_history_v2(all_results_v2)\n\n# 2. Sample predictions\nif all_results_v2:\n    visualize_sample_predictions_v2(test_loader, all_results_v2, num_samples=3)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"âœ… VERSION 2 VISUALIZATIONS COMPLETED!\")\nprint(\"=\"*60)\nprint(f\"ğŸ“ Visualizations saved to: {vis_dir}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}